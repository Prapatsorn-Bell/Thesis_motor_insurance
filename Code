# data cleaning and prepocessinh 

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, learning_curve
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import OLSInfluence


# 1. Data Cleaning: Handle missing values, drop columns, and clean up the dataset
def clean_data(df):
    date_columns = ['Date_birth', 'Date_start_contract', 'Date_last_renewal', 
                    'Date_next_renewal', 'Date_driving_licence', 'Date_lapse']
    df[date_columns] = df[date_columns].apply(pd.to_datetime, dayfirst=True, errors='coerce')
    drop_columns = ['Date_start_contract', 'Date_next_renewal', 
                    'Date_lapse', 'Length', 'Policies_in_force', 'Max_policies', 'Max_products']
    df = df.drop(columns=drop_columns)
    df['Distribution_channel'] = pd.to_numeric(
        df['Distribution_channel'].replace('00/01/1900', np.nan), errors='coerce'
    )
    df = df.dropna(subset=['Distribution_channel'])
    return df

# 2. Preprocessing: Filter data, apply conditions, and preprocess the columns
def preprocess_data(df):
    df = df[df['Type_risk'] == 3.0]
    df = df[(df["N_claims_year"] >= 1) & (df["N_claims_year"] <= 3)]
    return df

# 3. Feature Engineering: Create new features from existing data
def feature_engineering(df):
    # Create derived features (Driving experience and Age in 2018)
    df['Driving_experience_years'] = df['Date_last_renewal'].dt.year - df['Date_driving_licence'].dt.year
    df['Age_in_2018'] = df['Date_last_renewal'].dt.year - df['Date_birth'].dt.year
    # Group 'Age_in_2018' into age bins '16-24', '25-39', '40-64', '65+'
    age_bins = [16, 24, 39, 64, float('inf')]
    age_labels = ['16-24', '25-39', '40-64', '65+']
    df['Age_Group'] = pd.cut(df['Age_in_2018'], bins=age_bins, labels=age_labels, right=True)
    df = df.drop(columns=['Age_in_2018', 'Date_driving_licence', 'Date_birth', 'Date_last_renewal', 'Type_risk'])
    return df

# 4. One-hot Encoding using OneHotEncoder from sklearn
def one_hot_encode(df, categorical_cols):
    encoder = OneHotEncoder(sparse_output=False)
    encoded_cols = encoder.fit_transform(df[categorical_cols])
    encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))
    encoded_df.index = df.index
    df = df.drop(columns=categorical_cols).join(encoded_df)
    return df

# Main function to process motor data and evaluate models
def process_motor_data(file_path):
    df_motor = pd.read_csv(file_path, delimiter=';')
    df_motor = clean_data(df_motor)
    df_motor = preprocess_data(df_motor)
    df_motor = feature_engineering(df_motor)
    categorical_columns = ['Age_Group', 'Type_fuel']
    df_motor = one_hot_encode(df_motor, categorical_columns)
    return df_motor

df_processed_notexpand = process_motor_data('motor.csv')

# =======================================
# Multiplce classification 
# =======================================
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import auc, accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from imblearn.pipeline import Pipeline  # Import the pipeline from imbalanced-learn
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import joblib
from imblearn.over_sampling import SMOTE  # Import SMOTE


# Load data
train_resampled_set = pd.read_csv('train_set.csv')
test_set = pd.read_csv('test_set.csv')

X_train = train_resampled_set.drop(columns=['N_claims_year', 'ID'])
y_train = train_resampled_set['N_claims_year']
X_test = test_set.drop(columns=['N_claims_year', 'ID'])
y_test = test_set['N_claims_year']

# Define parameter grids for each model
param_grid_xgb = {
    'model__n_estimators': [50, 100, 200],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7],
    'model__subsample': [0.7, 0.8, 1.0],
    'model__colsample_bytree': [0.7, 0.8, 1.0]
}

# Updated function with pipeline for undersampling
def nested_cv_model(model, param_grid, model_name, X_train, y_train, X_test, y_test):
    print(f"\nStarting Nested Cross-Validation for {model_name}...")

    # Define the pipeline with undersampling and the model
    pipeline = Pipeline([
        ('over', SMOTE(random_state=42)),  # Use SMOTE here
        ('model', model)
    ])

    # Inner cross-validation for hyperparameter tuning using the pipeline
    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_grid,
        n_iter=10,
        scoring={'accuracy': 'accuracy', 'f1_weighted': 'f1_weighted'},
        refit='accuracy',
        cv=inner_cv,
        n_jobs=-1,
        random_state=42
    )

    # Outer cross-validation for model evaluation
    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Collect scores for each fold
    fold_results = []
    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X_train, y_train), start=1):
        print(f"\nFold {fold}...")

        # Split data for this fold
        X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]
        y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]

        # Print class distribution in the original outer fold's training subset before resampling
        print("Original class distribution in training fold:", Counter(y_train_fold))

        # Inner cross-validation with hyperparameter tuning and resampling handled by the pipeline
        search.fit(X_train_fold, y_train_fold)

        # Evaluate on the outer test fold
        y_pred_fold = search.predict(X_test_fold)
        accuracy_fold = accuracy_score(y_test_fold, y_pred_fold)
        f1_fold = f1_score(y_test_fold, y_pred_fold, average='weighted')
        roc_auc_fold = roc_auc_score(pd.get_dummies(y_test_fold), pd.get_dummies(y_pred_fold), average='macro', multi_class='ovr')

        # Print fold results
        print(f"  Fold {fold} Accuracy: {accuracy_fold}")
        print(f"  Fold {fold} F1 Score: {f1_fold}")
        print(f"  Fold {fold} ROC AUC Score: {roc_auc_fold}")

        fold_results.append((fold, accuracy_fold, f1_fold, roc_auc_fold))

        # Plot confusion matrix for each fold
        conf_matrix = confusion_matrix(y_test_fold, y_pred_fold)
        plt.figure(figsize=(8, 6))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
        plt.title(f"Confusion Matrix for {model_name} - Fold {fold}")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.savefig(f"{model_name}_conf_matrix_fold_{fold}.png")
        plt.close()

    # Train the best model on the entire training set
    search.fit(X_train, y_train)
    best_model = search.best_estimator_

    # Predictions on training and test sets
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)
    y_pred_probabilities = best_model.predict_proba(X_test)

    # Evaluation metrics on test set
    accuracy = accuracy_score(y_test, y_pred_test)
    f1 = f1_score(y_test, y_pred_test, average='weighted')
    roc_auc = roc_auc_score(pd.get_dummies(y_test), y_pred_probabilities, average='macro', multi_class='ovr')

    class_report = classification_report(y_test, y_pred_test)
    conf_matrix = confusion_matrix(y_test, y_pred_test)

    # Display results
    print(f"\n{model_name} Best Parameters (pipeline with undersampling): {search.best_params_}")
    print(f"{model_name} Accuracy on Test Set: {accuracy}")
    print(f"{model_name} F1 Score on Test Set: {f1}")
    print(f"{model_name} ROC AUC Score on Test Set: {roc_auc}")
    print(f"{model_name} Classification Report:\n{class_report}")

    # Plot final confusion matrix for the test set
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title(f"Final Confusion Matrix for {model_name} - Test Set (Undersampling)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.savefig(f"{model_name}_final_conf_matrix.png")
    plt.show()

    # Updated ROC curve plotting with bootstrapped confidence intervals
    plt.figure(figsize=(12, 8))
    colors = ['gold', 'royalblue', 'darkviolet', 'limegreen']

    y_test_dummies = pd.get_dummies(y_test)
    bootstraps = 1000  # Number of bootstrap samples
    mean_fpr = np.linspace(0, 1, 100)

    for i, class_label in enumerate(np.unique(y_test)):
        tprs = []

        for _ in range(bootstraps):
            indices = np.random.randint(0, len(y_test), len(y_test))
            y_true_bootstrap = y_test_dummies.iloc[indices, i]
            y_pred_bootstrap = y_pred_probabilities[indices, i]
            fpr, tpr, _ = roc_curve(y_true_bootstrap, y_pred_bootstrap)
            tpr_interp = np.interp(mean_fpr, fpr, tpr)
            tpr_interp[0] = 0.0
            tprs.append(tpr_interp)

        mean_tpr = np.mean(tprs, axis=0)
        mean_tpr[-1] = 1.0
        mean_auc = auc(mean_fpr, mean_tpr)
        std_tpr = np.std(tprs, axis=0)

        plt.plot(mean_fpr, mean_tpr, color=colors[i % len(colors)], label=f'Class {class_label} (Mean AUC = {mean_auc:.2%})')
        plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, color=colors[i % len(colors)], alpha=0.2)

    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title(f"ROC Curve with Confidence Intervals (Undersampling): {model_name}")
    plt.xlabel("1 - Specificity [False Positive Rate]")
    plt.ylabel("Sensitivity [True Positive Rate]")
    plt.legend(loc='lower right')
    plt.savefig(f"{model_name}_roc_curve_with_ci_undersampling.png")
    plt.show()

    # Export separate CSVs for train and test set predictions
    train_df = X_train.copy()
    train_df['y_true_train'] = y_train
    train_df['y_pred_train'] = y_pred_train

    test_df = X_test.copy()
    test_df['y_true_test'] = y_test
    test_df['y_pred_test'] = y_pred_test

    train_df.to_csv(f"{model_name.lower().replace(' ', '_')}_train_undersampling_pipe.csv", index=False)
    test_df.to_csv(f"{model_name.lower().replace(' ', '_')}_test_undersampling_pipe.csv", index=False)
    print(f"\n{model_name} train and test predictions have been saved separately.")

    # Save the model
    joblib.dump(best_model, f"{model_name}_best_model.joblib")

    return fold_results, best_model

# 18 mins Run the nested CV with the XGBoost model as an example
fold_results_xgb, best_model_xgb = nested_cv_model(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42), param_grid_xgb, 'XGBoost', X_train, y_train, X_test, y_test)






# =======================================
# Regression models 
# =======================================
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import OLSInfluence
from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, KFold, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats 

df_train = pd.read_csv('train_set_transformed_notlimit_2.csv')
df_test =  pd.read_csv('test_set_transformed_notlimit_2.csv')
# Function to calculate MAPE (Mean Absolute Percentage Error)
def mean_absolute_percentage_error(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true))

def run_model_with_top_features_and_qq_plot(df_train, df_test, model, param_grid, model_name="Model"):

    # Separate features and target variable in training and test datasets
    X_train = df_train.drop(columns=['Claim_costs', "ID"])  # Replace 'Claim_costs' if different
    y_train = df_train['Claim_costs']
    X_test = df_test.drop(columns=['Claim_costs', "ID"])
    y_test = df_test['Claim_costs']

    # Nested cross-validation with hyperparameter tuning
    def hyperparameter_tuning_with_nested_cv(model, param_grid, X_train, y_train):
        inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)
        outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)

        fold_metrics = []  # To store metrics for each fold

        for fold, (train_idx, valid_idx) in enumerate(outer_cv.split(X_train, y_train), start=1):
            X_fold_train, X_fold_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_fold_train, y_fold_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]

            # Run hyperparameter tuning on the training fold
            grid_search = RandomizedSearchCV(
                estimator=model,
                param_distributions=param_grid,
                cv=inner_cv,
                n_jobs=-1,
                scoring='neg_mean_squared_error',
                random_state=42
            )
            grid_search.fit(X_fold_train, y_fold_train)

            # Use the best model to predict the validation fold
            best_model = grid_search.best_estimator_
            y_pred_valid = best_model.predict(X_fold_valid)

            # Calculate metrics for the validation fold
            mse_fold = mean_squared_error(y_fold_valid, y_pred_valid)
            mae_fold = mean_absolute_error(y_fold_valid, y_pred_valid)

            # Print metrics for the fold
            print(f"Fold {fold} Results:")
            print(f"  Mean Squared Error (MSE): {mse_fold:.4f}")
            print(f"  Mean Absolute Error (MAE): {mae_fold:.4f}")

            # Store metrics for summary
            fold_metrics.append((fold, mse_fold, mae_fold))

        # After cross-validation, refit on the entire training set and return the best model
        grid_search.fit(X_train, y_train)
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_

        return best_model, best_params, fold_metrics

    # Run nested CV and get the best model and fold metrics
    best_model, best_params, fold_metrics = hyperparameter_tuning_with_nested_cv(
        model, param_grid, X_train, y_train
    )

    print(f"\nBest Parameters from Grid Search for {model_name}:\n{best_params}\n")

    # Summary of metrics across folds
    print(f"\n{model_name} Cross-Validation Summary:")
    for fold, mse_fold, mae_fold in fold_metrics:
        print(f"  Fold {fold}: MSE = {mse_fold:.4f}, MAE = {mae_fold:.4f}")

    # Evaluate the model on the full test set
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)
    residuals_train = y_train - y_pred_train
    residuals_test = y_test - y_pred_test

    # Calculate metrics for training and testing
    mse_train = mean_squared_error(y_train, y_pred_train)
    mae_train = mean_absolute_error(y_train, y_pred_train)
    r2_train = r2_score(y_train, y_pred_train)

    mse_test = mean_squared_error(y_test, y_pred_test)
    mae_test = mean_absolute_error(y_test, y_pred_test)
    r2_test = r2_score(y_test, y_pred_test)

    # Print evaluation results
    print(f"\n{model_name} Model Evaluation on Training Set:")
    print(f"Mean Squared Error (MSE - Training): {mse_train:.4f}")
    print(f"Mean Absolute Error (MAE - Training, Log Transformed): {mae_train:.4f}")
    print(f"R² Score (Training): {r2_train:.4f}")

    print(f"\n{model_name} Model Evaluation on Test Set:")
    print(f"Mean Squared Error (MSE - Testing): {mse_test:.4f}")
    print(f"Mean Absolute Error (MAE - Testing, Log Transformed): {mae_test:.4f}")
    print(f"R² Score (Testing): {r2_test:.4f}")

     # Convert log-transformed predictions and true values back to their original scale
    df_train['y_true'] = y_train.copy()  # Ensure it's a copy of y_test
    df_train['y_pred'] = y_pred_train
    df_train['y_pred_nolog'] = np.expm1(y_pred_train)  # Predicted values in original scale
    df_train['y_true_nolog'] = np.expm1(y_train) 
    mae_train_exp = mean_absolute_error(df_train['y_true_nolog'], df_train['y_pred_nolog'])
    mape_train_exp = mean_absolute_percentage_error(df_train['y_true_nolog'], df_train['y_pred_nolog'])
    print(f"Mean Absolute Percentage Error (MAPE - Training, Original Scale): {mape_train_exp:.4f}")
    print(f"Mean Absolute Error (MAE - Training, Original Scale): {mae_train_exp:.4f}")

    # Convert log-transformed predictions and true values back to their original scale
    df_test['y_true'] = y_test.copy()  # Ensure it's a copy of y_test
    df_test['y_pred'] = y_pred_test
    df_test['y_pred_nolog'] = np.expm1(y_pred_test)  # Predicted values in original scale
    df_test['y_true_nolog'] = np.expm1(y_test)     
    mae_test_exp = mean_absolute_error(df_test['y_true_nolog'], df_test['y_pred_nolog'])
    mape_test_exp = mean_absolute_percentage_error(df_test['y_true_nolog'], df_test['y_pred_nolog'])
    print(f"Mean Absolute Percentage Error (MAPE - Testing, Original Scale): {mape_test_exp:.4f}")
    print(f"Mean Absolute Error (MAE - Testing, Original Scale): {mae_test_exp:.4f}")

    # Store results in new DataFrames
    train_results = df_train.copy()
    test_results = df_test.copy()

    # Add predictions and residuals to DataFrames
    train_results['y_true'] = y_train
    train_results['y_pred'] = y_pred_train
    train_results['residuals'] = y_train - y_pred_train

    test_results['y_true'] = y_test
    test_results['y_pred'] = y_pred_test
    test_results['residuals'] = y_test - y_pred_test

    # Save results
    train_results.to_csv(f"{model_name.lower().replace(' ', '_')}_train_nocut_2_increase_tune.1K.csv", index=False)
    test_results.to_csv(f"{model_name.lower().replace(' ', '_')}_test_nocut_2_increase_tune1K.csv", index=False)


    # Plot Actual vs Predicted
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred_test, alpha=0.6, edgecolor='k', label='Predictions')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')
    plt.title(f"Actual vs. Predicted {model_name} Regressor", fontsize=14)
    plt.xlabel("Actual Cost Claims Year", fontsize=12)
    plt.ylabel("Predicted Cost Claims Year", fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.show()

     # Plot Actual vs Predicted using original scale
    plt.figure(figsize=(10, 6))
    plt.scatter(np.exp(y_test), np.exp(y_pred_test), alpha=0.6, edgecolor='k', label='Predictions')
    plt.plot([np.exp(y_test).min(), np.exp(y_test).max()], 
             [np.exp(y_test).min(), np.exp(y_test).max()], 'r--', lw=2, label='Perfect Prediction')
    plt.title(f"Actual vs. Predicted {model_name} Regressor (Original Scale)", fontsize=14)
    plt.xlabel("Actual Cost Claims Year (Original Scale)", fontsize=12)
    plt.ylabel("Predicted Cost Claims Year (Original Scale)", fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.show()

    # Plot Q-Q Plot for Residuals
    plt.figure(figsize=(8, 8))
    stats.probplot(residuals_test, dist="norm", plot=plt)
    plt.title(f"Q-Q Plot for {model_name} Residuals", fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()
    # Plot residuals and prediction density
    plot_residuals_and_qq(y_train, y_pred_train, y_test, y_pred_test, model_name)
    plot_residual_density(y_train, y_pred_train, y_test, y_pred_test, model_name)
    plot_prediction_kde(y_pred_train, y_pred_test, model_name)
    compute_cooks_and_leverage(X_test, y_test, y_pred_test, model_name, df_test)

    return best_model
# Density plot of residuals
def plot_residual_density(y_train_true, y_train_pred, y_test_true, y_test_pred, model_name):
    train_residuals, test_residuals = y_train_true - y_train_pred, y_test_true - y_test_pred
    plt.figure(figsize=(10, 6))
    sns.kdeplot(train_residuals, color='blue', fill=True, label='Train Residuals')
    sns.kdeplot(test_residuals, color='green', fill=True, label='Test Residuals')
    plt.title(f'Density Plot of Residuals for {model_name} (Claim Severity)')
    plt.xlabel('Residuals')
    plt.legend()
    plt.show()

# Density plot of predictions
def plot_prediction_kde(y_train_pred, y_test_pred, model_name):
    plt.figure(figsize=(10, 6))
    sns.kdeplot(y_train_pred, color='blue', fill=True, label='Train Predictions')
    sns.kdeplot(y_test_pred, color='green', fill=True, label='Test Predictions')
    plt.title(f'Kernel Density Plot of Predictions for {model_name} (Claim Severity)')
    plt.xlabel('Predicted Values')
    plt.legend()
    plt.show()

# Function to compute Cook's distance and leverage
def compute_cooks_and_leverage(X, y_true, y_pred, model_name, df):
    residuals = y_true - y_pred
    X_with_const = sm.add_constant(X)
    linear_model = sm.OLS(residuals, X_with_const).fit()
    influence = OLSInfluence(linear_model)
    leverage = influence.hat_matrix_diag
    cooks_d = influence.cooks_distance[0]
    leverage_threshold = 3 * np.mean(leverage)
    cooks_threshold = 4 / len(cooks_d)
    high_leverage_points = np.where(leverage > leverage_threshold)[0]
    high_cooks_points = np.where(cooks_d > cooks_threshold)[0]
    ids_high_leverage = df.loc[high_leverage_points, 'ID'].tolist()
    ids_high_cooks = df.loc[high_cooks_points, 'ID'].tolist()
    print(f"High leverage points IDs: {ids_high_leverage}")
    print(f"High Cook's distance points IDs: {ids_high_cooks}")
    return cooks_d, leverage, ids_high_leverage, ids_high_cooks

def plot_residuals_and_qq(y_train_true, y_train_pred, y_test_true, y_test_pred, model_name):
    train_residuals, test_residuals = y_train_true - y_train_pred, y_test_true - y_test_pred
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

    sns.scatterplot(x=y_train_pred, y=train_residuals, color='blue', ax=ax1, label=f'Train MAPE = {mean_absolute_percentage_error(y_train_true, y_train_pred):.3f}')
    sns.scatterplot(x=y_test_pred, y=test_residuals, color='green', ax=ax1, label=f'Test MAPE = {mean_absolute_percentage_error(y_test_true, y_test_pred):.3f}')
    ax1.axhline(0, color='black', linestyle='--')
    ax1.set_title(f'Residuals for {model_name} Model (Claim Severity)')
    stats.probplot(train_residuals, dist="norm", plot=ax2)
    stats.probplot(test_residuals, dist="norm", plot=ax2)
    plt.tight_layout()
    plt.show()

# =======================================
# SHAP
# =======================================
import shap

def shap_analysis_kernel(model, X_train, X_test, model_name):
    # Use a sample of X_train for background distribution to improve computation efficiency
    background = shap.sample(X_train, 100)
    
    # Initialize KernelExplainer
    explainer = shap.KernelExplainer(model.predict, background)
    
    # Compute SHAP values
    shap_values = explainer.shap_values(X_test, nsamples=100)  # Reduce nsamples if computation is slow
    
    # Summary Plot
    print(f"\nSHAP Summary Plot for {model_name}")
    shap.summary_plot(shap_values, X_test)
    
    # Bar Plot
    print(f"\nSHAP Bar Plot for {model_name}")
    shap.summary_plot(shap_values, X_test, plot_type="bar")
    
    # Dependence Plot for Top Feature
    top_feature = X_test.columns[np.abs(shap_values).mean(axis=0).argmax()]
    print(f"\nSHAP Dependence Plot for {top_feature} in {model_name}")
    shap.dependence_plot(top_feature, shap_values, X_test)

print("Running SHAP Analysis for Gradient Boosting using KernelExplainer...")
shap_analysis_kernel(best_gb, X_train, X_test, "Gradient Boosting Regressor")

# =======================================
# Boruta
# =======================================


# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from boruta import BorutaPy

df_train = pd.read_csv('train_set_transformed_notlimit_2.csv')
df_test =  pd.read_csv('test_set_transformed_notlimit_2.csv')

# Separate features and target variable
X_train = df_train.drop(columns=['Claim_costs', "ID", "Type_risk"])
y_train = df_train['Claim_costs']
X_test = df_test.drop(columns=['Claim_costs', "ID", "Type_risk"])
y_test = df_test['Claim_costs']


# ========================
# Boruta Feature Selection Function
# ========================
def boruta_feature_selection(X_train, y_train, model_type="RandomForest"):
    """
    Perform Boruta feature selection using a specified model.

    Args:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Target variable.
        model_type (str): Model type ('RandomForest', 'GradientBoosting', or 'XGBoost').

    Returns:
        Tuple: (Selected features DataFrame, List of selected feature names, DataFrame of feature rankings).
    """
    # Select the base model for Boruta
    if model_type == "RandomForest":
        base_model = RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=400, random_state=42)
    elif model_type == "GradientBoosting":
        base_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_leaf=2, min_samples_split=5, random_state=42)
    elif model_type == "XGBoost":
        base_model = XGBRegressor(subsample = 1.0, n_estimators=500, learning_rate=0.01, max_depth=8, colsample_bytree = 1.0, random_state=42)
    else:
        raise ValueError("Invalid model type. Choose either 'RandomForest', 'GradientBoosting', or 'XGBoost'")


    # Initialize Boruta with the tuned model
    boruta_selector = BorutaPy(
        estimator=base_model,
        n_estimators='auto',
        random_state=42,
        verbose=2
    )
    
    # Fit Boruta selector on the entire dataset
    boruta_selector.fit(X_train.values, y_train.values)
    
    # Extract selected features
    selected_features = X_train.columns[boruta_selector.support_].tolist()
    print(f"Selected Features by Boruta ({model_type}): {selected_features}")

    # Create a DataFrame for feature rankings
    feature_ranking = pd.DataFrame({
        'Feature': X_train.columns,
        'Ranking': boruta_selector.ranking_
    }).sort_values(by='Ranking')  # Sort by ranking (1 is the best rank)
    
    print(f"\nFeature Rankings by Boruta ({model_type}):")
    print(feature_ranking)
    
    return X_train[selected_features], selected_features, feature_ranking

# ========================
# Execute Boruta Feature Selection
# ========================
# Boruta for XGBoost
print("\nExecuting Boruta for XGBoost...")
X_train_xgb, selected_features_xgb, xgb_feature_ranking = boruta_feature_selection(X_train, y_train, model_type="XGBoost")















